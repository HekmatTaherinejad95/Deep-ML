{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to calculate the covariance matrix for a given set of vectors. The function should take a list of lists, where each inner list represents a feature with its observations, and return a covariance matrix as a list of lists. Additionally, provide test cases to verify the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, 3], [4, 5, 6]]\n",
    "out = [[1.0, 1.0], [1.0, 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "rows = len(data)\n",
    "cols = len(data[0]) if data else 0\n",
    "shape = (rows, cols)\n",
    "print(\"Shape of data:\", shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 5.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = []\n",
    "\n",
    "for row in data:\n",
    "    row_mean = sum(row) / len(row)\n",
    "    mean.append(row_mean)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(data)\n",
    "n_observations = len(data[0])\n",
    "\n",
    "means = [sum(feature) / n_observations for feature in data]\n",
    "\n",
    "cov_matrix = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_features):\n",
    "    row = []\n",
    "    for j in range(n_features):\n",
    "        cov = sum(\n",
    "            (data[i][k] - means[i]) * (data[j][k] - means[j])\n",
    "            for k in range(n_observations)\n",
    "        ) / (n_observations - 1)\n",
    "        row.append(cov)\n",
    "    cov_matrix.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 1.0], [1.0, 1.0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Linear Equations using Jacobi Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that uses the Jacobi method to solve a system of linear equations given by Ax = b. The function should iterate n times, rounding each intermediate solution to four decimal places, and return the approximate solution x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[5, -2, 3], [-3, 9, 1], [2, -1, -7]]\n",
    "b = [-1, 2, 3] \n",
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = [0.146, 0.2032, -0.5175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array(A, dtype=float)\n",
    "b = np.array(b, dtype=float)\n",
    "\n",
    "d = len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = [-0.2, 0.2222, -0.4286]\n",
      "Iteration 2: x = [0.146, 0.2032, -0.5175]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n):\n",
    "        x_new = np.zeros(d)\n",
    "        \n",
    "        for i in range(d):\n",
    "            # Calculate sum of off-diagonal terms: sum(a[i,j] * x[j] for j != i)\n",
    "            off_diagonal_sum = 0\n",
    "            for j in range(d):\n",
    "                if i != j:\n",
    "                    off_diagonal_sum += A[i, j] * x[j]\n",
    "            \n",
    "            # Update x[i] using the Jacobi formula\n",
    "            x_new[i] = (b[i] - off_diagonal_sum) / A[i, i]\n",
    "        \n",
    "        # Round to four decimal places\n",
    "        x_new = np.round(x_new, decimals=4)\n",
    "        \n",
    "        # Update x for next iteration\n",
    "        x = x_new\n",
    "        \n",
    "        print(f\"Iteration {iteration + 1}: x = {x.tolist()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function called svd_2x2_singular_values(A) that finds an approximate singular value decomposition of a real 2 x 2 matrix using one Jacobi rotation. Input A: a NumPy array of shape (2, 2)\n",
    "\n",
    "Rules You may use basic NumPy operations (matrix multiplication, transpose, element wise math, etc.). Do not call numpy.linalg.svd or any other high-level SVD routine. Stick to a single Jacobi step no iterative refinements.\n",
    "\n",
    "Return A tuple (U, √é¬£, V_T) where U is a 2 x 2 orthogonal matrix, √é¬£ is a length 2 NumPy array containing the singular values, and V_T is the transpose of the right-singular-vector matrix V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[2, 1], [1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: (array([[-0.70710678, -0.70710678],\n",
      "                [-0.70710678,  0.70710678]]),\n",
      "        array([3., 1.]),\n",
      "        array([[-0.70710678, -0.70710678],\n",
      "               [-0.70710678,  0.70710678]]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Output: (array([[-0.70710678, -0.70710678],\\n\"\n",
    "      \"                [-0.70710678,  0.70710678]]),\\n\"\n",
    "      \"        array([3., 1.]),\\n\"\n",
    "      \"        array([[-0.70710678, -0.70710678],\\n\"\n",
    "      \"               [-0.70710678,  0.70710678]]))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A_T = np.transpose(a)\n",
    "\n",
    "B = np.dot(A_T, A)\n",
    "\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853981633974483"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if B[0,0] == B[1,1]:\n",
    "    theta = np.pi/4\n",
    "else:\n",
    "    theta = np.arctan(2*B[0,1]/(B[0,0]-B[1,1]))/2\n",
    "theta\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678, -0.70710678],\n",
       "       [ 0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.00000000e+00, 1.11087808e-15],\n",
       "       [6.36938863e-16, 1.00000000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = R.T @ B @ R\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.sqrt([D[0,0], D[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]] [[3. 0.]\n",
      " [0. 1.]] [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "V = R\n",
    "Sigma_inv = np.diag(1/s)\n",
    "U = A @ V @ Sigma_inv\n",
    "U\n",
    "print(U, np.diag(s), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs linear regression using the normal equation. The function should take a matrix X (features) and a vector y (target) as input, and return the coefficients of the linear regression model. Round your answer to four decimal places, -0.0 is a valid result for rounding a very small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1, 1], [1, 2], [1, 3]]\n",
    "y = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.77635684e-15,  1.00000000e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "coefficients = np.round(coefficients, 4)\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the normal equation provides an exact solution for linear regression, there are several important reasons why gradient descent is often preferred in practice:\n",
    "\n",
    "## 1. **Computational Complexity**\n",
    "- **Normal Equation**: Requires computing `(X^T X)^(-1) X^T y`\n",
    "- **Matrix Inversion**: O(n¬≥) complexity where n is the number of features\n",
    "- **Memory Usage**: Needs to store the entire dataset in memory\n",
    "- **Gradient Descent**: O(n) per iteration, can handle much larger datasets\n",
    "\n",
    "## 2. **Scalability Issues**\n",
    "```python\n",
    "# Normal equation becomes impractical for large datasets\n",
    "# If you have 100,000 features, you need to invert a 100,000 √ó 100,000 matrix!\n",
    "# This would require ~80GB of RAM just for the matrix\n",
    "```\n",
    "\n",
    "## 3. **Non-linear Models**\n",
    "The normal equation only works for **linear regression**. Many real-world problems require:\n",
    "- **Logistic Regression**: Uses sigmoid function (non-linear)\n",
    "- **Neural Networks**: Multiple non-linear layers\n",
    "- **Polynomial Regression**: Higher-degree terms\n",
    "- **Regularized Models**: L1/L2 penalties\n",
    "\n",
    "## 4. **Online Learning**\n",
    "- **Normal Equation**: Requires all data at once (batch learning)\n",
    "- **Gradient Descent**: Can update parameters with each new data point (online learning)\n",
    "- **Stochastic GD**: Processes one example at a time\n",
    "\n",
    "## 5. **Numerical Stability**\n",
    "```python\n",
    "# Normal equation can be numerically unstable\n",
    "# If X^T X is nearly singular (multicollinearity), \n",
    "# the inverse becomes unreliable\n",
    "```\n",
    "\n",
    "## 6. **Feature Scaling**\n",
    "- **Normal Equation**: Works regardless of feature scales\n",
    "- **Gradient Descent**: Requires feature scaling for optimal convergence\n",
    "\n",
    "## When to Use Each:\n",
    "\n",
    "**Use Normal Equation when:**\n",
    "- Small dataset (< 10,000 examples)\n",
    "- Few features (< 1,000)\n",
    "- Linear regression only\n",
    "- Exact solution needed\n",
    "\n",
    "**Use Gradient Descent when:**\n",
    "- Large datasets\n",
    "- Many features\n",
    "- Non-linear models\n",
    "- Online learning needed\n",
    "- Memory constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs linear regression using gradient descent. The function should take NumPy arrays X (features with a column of ones for the intercept) and y (target) as input, along with learning rate alpha and the number of iterations, and return the coefficients of the linear regression model as a NumPy array. Round your answer to four decimal places. -0.0 is a valid result for rounding a very small number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3]) \n",
    "alpha = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array([0.1107, 0.9513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def linear_regression_gd(X, y, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs linear regression using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        X : np.ndarray, shape (m, n)\n",
    "            Feature matrix with a column of ones for the intercept.\n",
    "        y : np.ndarray, shape (m,)\n",
    "            Target vector.\n",
    "        alpha : float\n",
    "            Learning rate.\n",
    "        iterations : int\n",
    "            Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        theta : np.ndarray\n",
    "            Coefficients of the linear regression model, rounded to 4 decimals.\n",
    "    \"\"\"\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "    \n",
    "    # Get dimensions\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize parameters\n",
    "    theta = np.zeros((n, 1))\n",
    "    \n",
    "    # Gradient descent\n",
    "    for _ in range(iterations):\n",
    "        h = X @ theta  # Hypothesis: X @ theta\n",
    "        gradient = (1/m) * (X.T @ (h - y))  # Gradient of cost function\n",
    "        theta = theta - alpha * gradient  # Update parameters\n",
    "    \n",
    "    # Return flattened and rounded coefficients\n",
    "    return np.round(theta.flatten(), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs feature scaling on a dataset using both standardization and min-max normalization. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. It should return two 2D NumPy arrays: one scaled by standardization and one by min-max normalization. Make sure all results are rounded to the nearest 4th decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2], [3, 4], [5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.63299316, 1.63299316])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]] [[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "min_max_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n",
    "print(standardized_data, min_max_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to write a Python function that implements the k-Means clustering algorithm. This function should take specific inputs and produce a list of final centroids. k-Means clustering is a method used to partition n points into k clusters. The goal is to group similar points together and represent each group by its center (called the centroid).\n",
    "Function Inputs:\n",
    "\n",
    "    points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n",
    "    k: An integer representing the number of clusters to form\n",
    "    initial_centroids: A list of initial centroid points, each a tuple of coordinates\n",
    "    max_iterations: An integer representing the maximum number of iterations to perform\n",
    "\n",
    "Function Output:\n",
    "\n",
    "A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)]\n",
    "k = 2\n",
    "initial_centroids = [(1, 1), (10, 1)] \n",
    "max_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(max_iterations):\n",
    "    clusters = [[] for _ in range(k)]\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array(points[0]) - np.array(initial_centroids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(max_iterations):\n",
    "    clusters = [[] for _ in range(k)]\n",
    "    for point in points:\n",
    "        distances = [np.linalg.norm(np.array(point) - np.array(centroid)) for centroid in initial_centroids]\n",
    "        cluster_index = np.argmin(distances)\n",
    "        clusters[cluster_index].append(point)\n",
    "    new_centroids = [np.mean(cluster, axis=0) for cluster in clusters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function to generate train and test splits for K-Fold Cross-Validation. Your task is to divide the dataset into k folds and return a list of train-test indices for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "y = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "k = 5\n",
    "shuffle = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(len(X))\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1, 5, 0, 7, 2, 9, 4, 3, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.RandomState(seed= 42)\n",
    "rng.shuffle(indices)\n",
    "indices\n",
    "# We shuffle to ensure that each fold in cross-validation is representative of the overall dataset, preventing bias from any \n",
    "# inherent ordering in the data and improving the reliability of model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_sizes = np.full(k, len(X) // k, dtype=int)\n",
    "fold_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_sizes[:len(X) % k] += 1\n",
    "fold_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n",
    "    \n",
    "    assert len(X) == len(y)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    indices = np.arange(n_samples)\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(seed= random_seed)\n",
    "        rng.shuffle(indices)\n",
    "    \n",
    "    fold_sizes = np.full(k, n_samples // k, dtype=int)\n",
    "    fold_sizes[:n_samples % k] += 1\n",
    "\n",
    "    current_index = 0\n",
    "    splits = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start = current_index\n",
    "        end = current_index + fold_size\n",
    "        test_indices = indices[start:end]\n",
    "        train_indices = np.concatenate((indices[:start], indices[end:]))\n",
    "        splits.append((train_indices, test_indices))\n",
    "        current_index = end\n",
    "        \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2, 3, 4, 5, 6, 7, 8, 9]), array([0, 1])),\n",
       " (array([0, 1, 4, 5, 6, 7, 8, 9]), array([2, 3])),\n",
       " (array([0, 1, 2, 3, 6, 7, 8, 9]), array([4, 5])),\n",
       " (array([0, 1, 2, 3, 4, 5, 8, 9]), array([6, 7])),\n",
       " (array([0, 1, 2, 3, 4, 5, 6, 7]), array([8, 9]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs Principal Component Analysis (PCA) from scratch. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. The function should standardize the dataset, compute the covariance matrix, find the eigenvalues and eigenvectors, and return the principal components (the eigenvectors corresponding to the largest eigenvalues). The function should also take an integer k as input, representing the number of principal components to return.\n",
    "\n",
    "\n",
    "    1- Standardize the Dataset: Ensure that each feature has a mean of 0 and a standard deviation of 1.\n",
    "    2- Compute the Covariance Matrix: Reflects how features vary together.\n",
    "    3- Find Eigenvalues and Eigenvectors: Solve the characteristic equation for the covariance matrix.\n",
    "    4- Select Principal Components: Choose eigenvectors (components) with the highest eigenvalues for dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 4.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean(data, axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5, 1.5],\n",
       "       [1.5, 1.5]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data_centered = (data - mean) / std\n",
    "cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "top_k_indices = np.argsort(eigenvalues)[::-1][:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70710678, 0.70710678])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = eigenvectors[:, top_k_indices]\n",
    "principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that implements the decision tree learning algorithm for classification. The function should use recursive binary splitting based on entropy and information gain to build a decision tree. It should take a list of examples (each example is a dict of attribute-value pairs) and a list of attribute names as input, and return a nested dictionary representing the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "\n",
    "    1- Select Attribute: Choose the attribute with the highest information gain.\n",
    "    2- Split Dataset: Divide the dataset based on the values of the selected attribute.\n",
    "    3- Recursion: Repeat the process for each subset until:\n",
    "            All data is perfectly classified, or\n",
    "            No remaining attributes can be used to make a split.\n",
    "\n",
    "This recursive process continues until the decision tree can no longer be split further or all examples have been classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n",
    "                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n",
    "                    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n",
    "                    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n",
    "                ]\n",
    "\n",
    "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
    "target_attr = 'PlayTennis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'No', 'Yes', 'Yes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [ex[target_attr] for ex in examples]\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(values)\n",
    "\n",
    "for count in counts.values():\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(examples, target_attr):\n",
    "    values = [ex[target_attr] for ex in examples]\n",
    "    total = len(values)\n",
    "    counts = Counter(values)\n",
    "    ent = 0.0\n",
    "\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log2(p)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(examples, target_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overcast', 'Rain', 'Sunny'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = set(ex[attributes[0]] for ex in examples)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(examples, attr, target_attr):\n",
    "    total_entropy = entropy(examples, target_attr)\n",
    "    values = set(ex[attr] for ex in examples)\n",
    "    total = len(examples)\n",
    "    subset_entropy = 0.0\n",
    "    for v in values:\n",
    "        subset = [ex for ex in examples if ex[attr] == v]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def entropy(examples, target_attr):\n",
    "    values = [ex[target_attr] for ex in examples]\n",
    "    total = len(values)\n",
    "    counts = Counter(values)\n",
    "    ent = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def information_gain(examples, attr, target_attr):\n",
    "    total_entropy = entropy(examples, target_attr)\n",
    "    values = set(ex[attr] for ex in examples)\n",
    "    total = len(examples)\n",
    "    subset_entropy = 0.0\n",
    "    for v in values:\n",
    "        subset = [ex for ex in examples if ex[attr] == v]\n",
    "        weight = len(subset) / total\n",
    "        subset_entropy += weight * entropy(subset, target_attr)\n",
    "    return total_entropy - subset_entropy\n",
    "\n",
    "def majority_value(examples, target_attr):\n",
    "    values = [ex[target_attr] for ex in examples]\n",
    "    return Counter(values).most_common(1)[0][0]\n",
    "\n",
    "def all_same_class(examples, target_attr):\n",
    "    first = examples[0][target_attr]\n",
    "    return all(ex[target_attr] == first for ex in examples)\n",
    "\n",
    "def learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n",
    "    # Base cases\n",
    "    if not examples:\n",
    "        return None\n",
    "    if all_same_class(examples, target_attr):\n",
    "        return examples[0][target_attr]\n",
    "    if not attributes:\n",
    "        return majority_value(examples, target_attr)\n",
    "\n",
    "    # Choose best attribute\n",
    "    gains = [(attr, information_gain(examples, attr, target_attr)) for attr in attributes]\n",
    "    best_attr = max(gains, key=lambda x: x[1])[0]\n",
    "\n",
    "    tree = {best_attr: {}}\n",
    "    attr_values = set(ex[best_attr] for ex in examples)\n",
    "    for v in attr_values:\n",
    "        subset = [ex for ex in examples if ex[best_attr] == v]\n",
    "        if not subset:\n",
    "            subtree = majority_value(examples, target_attr)\n",
    "        else:\n",
    "            remaining_attrs = [a for a in attributes if a != best_attr]\n",
    "            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n",
    "        tree[best_attr][v] = subtree\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outlook': {'Sunny': 'No', 'Rain': 'Yes', 'Overcast': 'Yes'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_decision_tree(examples, attributes, 'PlayTennis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hot', 'Hot', 'Hot', 'Mild']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [ex['Temperature'] for ex in examples]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # üìö Import the math module for mathematical functions (e.g., log2)\n",
    "from collections import Counter  # üìä Import Counter to count occurrences of items\n",
    "from typing import List, Dict, Any, Union  # üìù Import type hints for better code clarity\n",
    "\n",
    "def calculate_entropy(labels: List[Any]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Shannon entropy of the list of labels.\n",
    "    Entropy formula: H(S) = -‚àë p_i * log2(p_i)\n",
    "    where p_i is the probability of class i.\n",
    "    \"\"\"\n",
    "    total = len(labels)  # üî¢ Total number of labels (N)\n",
    "    if total == 0:  # ‚ö†Ô∏è If there are no labels, entropy is 0\n",
    "        return 0.0\n",
    "    counts = Counter(labels)  # üßÆ Count occurrences of each label\n",
    "    entropy = 0.0  # üèÅ Initialize entropy to 0\n",
    "    for count in counts.values():  # üîÅ For each unique label count\n",
    "        p = count / total  # üìê Calculate probability p_i = count / N\n",
    "        if p > 0:  # üö¶ Only consider non-zero probabilities\n",
    "            entropy -= p * math.log2(p)  # ‚ûñ Add -p_i * log2(p_i) to entropy (Shannon entropy formula)\n",
    "    return entropy  # üéØ Return the computed entropy\n",
    "\n",
    "def calculate_information_gain(\n",
    "    examples: List[Dict[str, Any]],\n",
    "    attr: str,\n",
    "    target_attr: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute information gain for splitting `examples` on `attr` w.r.t. `target_attr`.\n",
    "    Information Gain formula: IG(S, A) = H(S) - ‚àë_v (|S_v|/|S|) * H(S_v)\n",
    "    where S is the set of examples, A is the attribute, v are possible values of A,\n",
    "    and S_v is the subset where A = v.\n",
    "    \"\"\"\n",
    "    total = len(examples)  # üî¢ Total number of examples (|S|)\n",
    "    if total == 0:  # ‚ö†Ô∏è If no examples, information gain is 0\n",
    "        return 0.0\n",
    "    labels = [ex[target_attr] for ex in examples]  # üè∑Ô∏è Extract all target labels\n",
    "    total_entropy = calculate_entropy(labels)  # üßÆ Compute H(S), the entropy before split\n",
    "    values = set(ex[attr] for ex in examples)  # üóÇÔ∏è Get all unique values of attribute A\n",
    "    subset_entropy = 0.0  # üèÅ Initialize weighted entropy after split\n",
    "    for v in values:  # üîÅ For each possible value v of attribute A\n",
    "        subset = [ex for ex in examples if ex[attr] == v]  # üì¶ Subset S_v where A = v\n",
    "        weight = len(subset) / total  # ‚öñÔ∏è Weight = |S_v| / |S|\n",
    "        subset_labels = [ex[target_attr] for ex in subset]  # üè∑Ô∏è Labels in subset S_v\n",
    "        subset_entropy += weight * calculate_entropy(subset_labels)  # ‚ûï Add weighted entropy: (|S_v|/|S|) * H(S_v)\n",
    "    return total_entropy - subset_entropy  # üèÜ Information gain: IG(S, A) = H(S) - weighted sum\n",
    "\n",
    "def majority_class(\n",
    "    examples: List[Dict[str, Any]],\n",
    "    target_attr: str\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Return the most common value of `target_attr` in `examples`.\n",
    "    (Used for majority voting in leaves or when no attributes left)\n",
    "    \"\"\"\n",
    "    labels = [ex[target_attr] for ex in examples]  # üè∑Ô∏è Extract all target labels\n",
    "    if not labels:  # ‚ö†Ô∏è If no labels, return None\n",
    "        return None\n",
    "    return Counter(labels).most_common(1)[0][0]  # ü•á Return the label with highest count\n",
    "\n",
    "def learn_decision_tree(\n",
    "    examples: List[Dict[str, Any]],\n",
    "    attributes: List[str],\n",
    "    target_attr: str\n",
    ") -> Union[Dict[str, Any], Any]:\n",
    "    \"\"\"\n",
    "    Learn a decision tree using the ID3 algorithm üå≥.\n",
    "    Returns either a nested dict representing the tree or a class label at the leaves.\n",
    "    \"\"\"\n",
    "    if not examples:  # üà≥ Base case: if no examples, return None\n",
    "        return None\n",
    "    labels = [ex[target_attr] for ex in examples]  # üè∑Ô∏è Get all target labels\n",
    "    if all(label == labels[0] for label in labels):  # ‚úÖ Base case: if all labels are the same, return that label (pure node)\n",
    "        return labels[0]\n",
    "    if not attributes:  # üèÅ Base case: if no attributes left, return majority class (majority voting)\n",
    "        return majority_class(examples, target_attr)\n",
    "    # üßÆ Compute information gain for each attribute (select best split)\n",
    "    gains = [(attr, calculate_information_gain(examples, attr, target_attr)) for attr in attributes]\n",
    "    best_attr = max(gains, key=lambda x: x[1])[0]  # ü•á Choose attribute with highest information gain\n",
    "    tree = {best_attr: {}}  # üå≥ Create a new decision node for best_attr\n",
    "    values = set(ex[best_attr] for ex in examples)  # üóÇÔ∏è Get all unique values for best_attr\n",
    "    for v in values:  # üîÅ For each value v of best_attr\n",
    "        subset = [ex for ex in examples if ex[best_attr] == v]  # üì¶ Subset where best_attr == v\n",
    "        if not subset:  # üà≥ If subset is empty, use majority class as leaf\n",
    "            subtree = majority_class(examples, target_attr)\n",
    "        else:\n",
    "            remaining_attrs = [a for a in attributes if a != best_attr]  # üìù Remove best_attr from attribute list\n",
    "            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)  # üîÑ Recursively build subtree\n",
    "        tree[best_attr][v] = subtree  # üåø Attach subtree to the current node for value v\n",
    "    return tree  # üå≥ Return the constructed decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outlook': {'Sunny': 'No', 'Rain': 'Yes', 'Overcast': 'Yes'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_decision_tree(examples, attributes, 'PlayTennis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasos Kernel SVM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch. The function should take a dataset (as a 2D NumPy array where each row represents a data sample and each column represents a feature), a label vector (1D NumPy array where each entry corresponds to the label of the sample), and training parameters such as the choice of kernel (linear or RBF), regularization parameter (lambda), and the number of iterations. Note that while the original Pegasos algorithm is stochastic (it selects a single random sample at each step), this problem requires using all samples in every iteration (i.e., no random sampling). The function should perform binary classification and return the model's alpha coefficients and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2], [2, 3], [3, 1], [4, 1]]) \n",
    "labels = np.array([1, 1, -1, -1]) \n",
    "kernel = 'rbf' \n",
    "lambda_val = 0.01 \n",
    "iterations = 100\n",
    "sigma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deterministic Pegasos Algorithm Steps**\n",
    "\n",
    "**Given:**\n",
    "- Training samples $(\\mathbf{x}_i, y_i)$, with $y_i \\in \\{-1, 1\\}$\n",
    "- Kernel function $K$\n",
    "- Regularization parameter $\\lambda$\n",
    "- Total iterations $T$\n",
    "\n",
    "---\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. **Initialize:**  \n",
    "   $\\alpha_i = 0$ for all $i$, and bias $b = 0$.\n",
    "\n",
    "2. **For each iteration** $t = 1, 2, \\ldots, T$:\n",
    "    - (a) **Compute learning rate:**  \n",
    "      $\\eta_t = \\frac{1}{\\lambda t}$\n",
    "    - (b) **For each training sample** $(\\mathbf{x}_i, y_i)$:\n",
    "        - i. **Compute decision value:**  \n",
    "          $f(\\mathbf{x}_i) = \\sum_j \\alpha_j y_j K(\\mathbf{x}_j, \\mathbf{x}_i) + b$\n",
    "        - ii. **If margin constraint is violated** ($y_i f(\\mathbf{x}_i) < 1$), **update:**\n",
    "            - $\\alpha_i \\leftarrow \\alpha_i + \\eta_t (y_i - \\lambda \\alpha_i)$\n",
    "            - $b \\leftarrow b + \\eta_t y_i$\n",
    "\n",
    "---\n",
    "\n",
    "**Common kernel functions:**\n",
    "\n",
    "- **Linear kernel:**  \n",
    "  $K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y}$\n",
    "\n",
    "- **Radial Basis Function (RBF) kernel:**  \n",
    "  $K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{y}\\|^2}{2\\sigma^2}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_function(x, y, kernel='linear', sigma=1.0):\n",
    "    if kernel == 'linear':\n",
    "        return np.dot(x, y)\n",
    "    elif kernel == 'rbf':\n",
    "        return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported kernel: {kernel}\")\n",
    "\n",
    "def pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0) -> (list, float):\n",
    "    n_samples = len(data)\n",
    "    alphas = [0.0] * n_samples\n",
    "    b = 0.0\n",
    "\n",
    "    for t in range(1, iterations + 1):\n",
    "        eta_t = 1 / (lambda_val * t)\n",
    "        for i in range(n_samples):\n",
    "            x_i = data[i]\n",
    "            y_i = labels[i]\n",
    "            # Use the correct kernel function\n",
    "            def kf(x, y):\n",
    "                return kernel_function(x, y, kernel=kernel, sigma=sigma)\n",
    "            # Compute decision value\n",
    "            decision_value = sum(alphas[j] * labels[j] * kf(data[j], x_i) for j in range(n_samples)) + b\n",
    "            if y_i * decision_value < 1:\n",
    "                # Update alpha_i and b as per the deterministic Pegasos update\n",
    "                alphas[i] += eta_t * (y_i - lambda_val * alphas[i])\n",
    "                b += eta_t * y_i\n",
    "            # No else branch: only update when margin is violated\n",
    "\n",
    "    # For output formatting, round b to 4 decimals as in the expected output\n",
    "    b = round(b, 4)\n",
    "    # If all alphas are integer-valued, cast to int for output\n",
    "    alphas_out = [float(a) if abs(a - int(a)) > 1e-8 else int(a) for a in alphas]\n",
    "    return alphas_out, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that computes the output of the sigmoid activation function given an input value z. The function should return the output rounded to four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def sigmoid(z: float) -> float:\n",
    "    result = 1 / (1 + math.exp(-z))\n",
    "    result = round(result, 4)\n",
    "    return result\n",
    "\n",
    "sigmoid(z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that computes the softmax activation for a given list of scores. The function should return the softmax values as a list, each rounded to four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softmax(scores: list[float]) -> list[float]:\n",
    "\t# Your code here\n",
    "\n",
    "    probabilities = []\n",
    "\n",
    "    sum_exp = sum(math.exp(score) for score in scores)\n",
    "\n",
    "    for z in scores:\n",
    "        softmax = math.exp(z) / sum_exp\n",
    "        softmax = round(softmax, 4)\n",
    "        probabilities.append(softmax) \n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features. The function should take a list of feature vectors (each vector representing multiple features for an example), associated true binary labels, and the neuron's weights (one for each feature) and bias as input. It should return the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]] \n",
    "labels = [0, 1, 0] \n",
    "weights = [0.7, -0.4] \n",
    "bias = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.050000000000000044"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(w * x for w, x in zip(weights, features[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n",
    "\t# Your code here\n",
    "\tprobabilities = []\n",
    "\tmse = 0\n",
    "\tfor feature, label in zip(features, labels):\n",
    "\t\tz = sum(w * x for w, x in zip(weights, feature)) + bias\n",
    "\t\tsigmoid = 1 / (1 + math.exp(-z))\n",
    "\t\tprobabilities.append(sigmoid)\n",
    "\t\tmse += (sigmoid - label) ** 2\n",
    "\tmse = mse / len(features)\n",
    "\treturn probabilities, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias. The function should take a list of feature vectors, associated true binary labels, initial weights, initial bias, a learning rate, and the number of epochs. The function should update the weights and bias using gradient descent based on the MSE loss, and return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Learning with Backpropagation\n",
    "\n",
    "This task involves implementing backpropagation for a single neuron in a neural network. The neuron processes inputs and updates its parameters to minimize the Mean Squared Error (MSE) between predicted outputs and true labels.\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "Compute the neuron's output by calculating the dot product of the weights and input features, then adding the bias:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "Apply the sigmoid activation function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### Loss Calculation (MSE)\n",
    "\n",
    "The Mean Squared Error quantifies the error between the neuron's predictions and the actual labels:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "### Backward Pass (Gradient Calculation)\n",
    "\n",
    "Compute the gradient of the MSE with respect to each weight and the bias. This involves the partial derivatives of the loss function with respect to the neuron's output, multiplied by the derivative of the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i) \\cdot \\sigma'(z_i) \\cdot x_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i) \\cdot \\sigma'(z_i)\n",
    "$$\n",
    "\n",
    "where $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$.\n",
    "\n",
    "### Parameter Update\n",
    "\n",
    "Update each weight and the bias by subtracting a portion of the gradient, determined by the learning rate $\\alpha$:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial \\text{MSE}}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial \\text{MSE}}{\\partial b}\n",
    "$$\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "This process refines the neuron's ability to predict accurately by iteratively adjusting the weights and bias based on the error gradients, optimizing the neural network's performance over multiple iterations (epochs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n",
    "\t# Your code here\n",
    "\tweights = initial_weights.copy()\n",
    "\tbias = initial_bias\n",
    "\tmse_values = []\n",
    "\tn_samples = features.shape[0]\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tz = np.dot(features, weights) + bias\n",
    "\t\tsigmoid = 1 / (1 + np.exp(-z))\n",
    "\t\terrors = sigmoid - labels\n",
    "\t\tmse = np.mean(errors ** 2)\n",
    "\t\tmse_values.append(mse)\n",
    "\n",
    "\t\t# Gradients\n",
    "\t\tsigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "\t\tgrad_w = (2 / n_samples) * np.dot((errors * sigmoid_deriv), features)\n",
    "\t\tgrad_b = (2 / n_samples) * np.sum(errors * sigmoid_deriv)\n",
    "\n",
    "\t\t# Update\n",
    "\t\tweights = weights - learning_rate * grad_w\n",
    "\t\tbias = bias - learning_rate * grad_b\n",
    "\n",
    "\tupdated_weights = weights\n",
    "\tupdated_bias = bias\n",
    "\treturn updated_weights, updated_bias, mse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
