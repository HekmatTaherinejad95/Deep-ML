{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to calculate the covariance matrix for a given set of vectors. The function should take a list of lists, where each inner list represents a feature with its observations, and return a covariance matrix as a list of lists. Additionally, provide test cases to verify the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2, 3], [4, 5, 6]]\n",
    "out = [[1.0, 1.0], [1.0, 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "rows = len(data)\n",
    "cols = len(data[0]) if data else 0\n",
    "shape = (rows, cols)\n",
    "print(\"Shape of data:\", shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 5.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = []\n",
    "\n",
    "for row in data:\n",
    "    row_mean = sum(row) / len(row)\n",
    "    mean.append(row_mean)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(data)\n",
    "n_observations = len(data[0])\n",
    "\n",
    "means = [sum(feature) / n_observations for feature in data]\n",
    "\n",
    "cov_matrix = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_features):\n",
    "    row = []\n",
    "    for j in range(n_features):\n",
    "        cov = sum(\n",
    "            (data[i][k] - means[i]) * (data[j][k] - means[j])\n",
    "            for k in range(n_observations)\n",
    "        ) / (n_observations - 1)\n",
    "        row.append(cov)\n",
    "    cov_matrix.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 1.0], [1.0, 1.0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Linear Equations using Jacobi Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that uses the Jacobi method to solve a system of linear equations given by Ax = b. The function should iterate n times, rounding each intermediate solution to four decimal places, and return the approximate solution x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[5, -2, 3], [-3, 9, 1], [2, -1, -7]]\n",
    "b = [-1, 2, 3] \n",
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = [0.146, 0.2032, -0.5175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array(A, dtype=float)\n",
    "b = np.array(b, dtype=float)\n",
    "\n",
    "d = len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = [-0.2, 0.2222, -0.4286]\n",
      "Iteration 2: x = [0.146, 0.2032, -0.5175]\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n):\n",
    "        x_new = np.zeros(d)\n",
    "        \n",
    "        for i in range(d):\n",
    "            # Calculate sum of off-diagonal terms: sum(a[i,j] * x[j] for j != i)\n",
    "            off_diagonal_sum = 0\n",
    "            for j in range(d):\n",
    "                if i != j:\n",
    "                    off_diagonal_sum += A[i, j] * x[j]\n",
    "            \n",
    "            # Update x[i] using the Jacobi formula\n",
    "            x_new[i] = (b[i] - off_diagonal_sum) / A[i, i]\n",
    "        \n",
    "        # Round to four decimal places\n",
    "        x_new = np.round(x_new, decimals=4)\n",
    "        \n",
    "        # Update x for next iteration\n",
    "        x = x_new\n",
    "        \n",
    "        print(f\"Iteration {iteration + 1}: x = {x.tolist()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function called svd_2x2_singular_values(A) that finds an approximate singular value decomposition of a real 2 x 2 matrix using one Jacobi rotation. Input A: a NumPy array of shape (2, 2)\n",
    "\n",
    "Rules You may use basic NumPy operations (matrix multiplication, transpose, element wise math, etc.). Do not call numpy.linalg.svd or any other high-level SVD routine. Stick to a single Jacobi step no iterative refinements.\n",
    "\n",
    "Return A tuple (U, √é¬£, V_T) where U is a 2 x 2 orthogonal matrix, √é¬£ is a length 2 NumPy array containing the singular values, and V_T is the transpose of the right-singular-vector matrix V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[2, 1], [1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: (array([[-0.70710678, -0.70710678],\n",
      "                [-0.70710678,  0.70710678]]),\n",
      "        array([3., 1.]),\n",
      "        array([[-0.70710678, -0.70710678],\n",
      "               [-0.70710678,  0.70710678]]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Output: (array([[-0.70710678, -0.70710678],\\n\"\n",
    "      \"                [-0.70710678,  0.70710678]]),\\n\"\n",
    "      \"        array([3., 1.]),\\n\"\n",
    "      \"        array([[-0.70710678, -0.70710678],\\n\"\n",
    "      \"               [-0.70710678,  0.70710678]]))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 4],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A_T = np.transpose(a)\n",
    "\n",
    "B = np.dot(A_T, A)\n",
    "\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853981633974483"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if B[0,0] == B[1,1]:\n",
    "    theta = np.pi/4\n",
    "else:\n",
    "    theta = np.arctan(2*B[0,1]/(B[0,0]-B[1,1]))/2\n",
    "theta\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678, -0.70710678],\n",
       "       [ 0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.00000000e+00, 1.11087808e-15],\n",
       "       [6.36938863e-16, 1.00000000e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = R.T @ B @ R\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.sqrt([D[0,0], D[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]] [[3. 0.]\n",
      " [0. 1.]] [[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "V = R\n",
    "Sigma_inv = np.diag(1/s)\n",
    "U = A @ V @ Sigma_inv\n",
    "U\n",
    "print(U, np.diag(s), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs linear regression using the normal equation. The function should take a matrix X (features) and a vector y (target) as input, and return the coefficients of the linear regression model. Round your answer to four decimal places, -0.0 is a valid result for rounding a very small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1, 1], [1, 2], [1, 3]]\n",
    "y = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output = [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.77635684e-15,  1.00000000e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "coefficients = np.round(coefficients, 4)\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the normal equation provides an exact solution for linear regression, there are several important reasons why gradient descent is often preferred in practice:\n",
    "\n",
    "## 1. **Computational Complexity**\n",
    "- **Normal Equation**: Requires computing `(X^T X)^(-1) X^T y`\n",
    "- **Matrix Inversion**: O(n¬≥) complexity where n is the number of features\n",
    "- **Memory Usage**: Needs to store the entire dataset in memory\n",
    "- **Gradient Descent**: O(n) per iteration, can handle much larger datasets\n",
    "\n",
    "## 2. **Scalability Issues**\n",
    "```python\n",
    "# Normal equation becomes impractical for large datasets\n",
    "# If you have 100,000 features, you need to invert a 100,000 √ó 100,000 matrix!\n",
    "# This would require ~80GB of RAM just for the matrix\n",
    "```\n",
    "\n",
    "## 3. **Non-linear Models**\n",
    "The normal equation only works for **linear regression**. Many real-world problems require:\n",
    "- **Logistic Regression**: Uses sigmoid function (non-linear)\n",
    "- **Neural Networks**: Multiple non-linear layers\n",
    "- **Polynomial Regression**: Higher-degree terms\n",
    "- **Regularized Models**: L1/L2 penalties\n",
    "\n",
    "## 4. **Online Learning**\n",
    "- **Normal Equation**: Requires all data at once (batch learning)\n",
    "- **Gradient Descent**: Can update parameters with each new data point (online learning)\n",
    "- **Stochastic GD**: Processes one example at a time\n",
    "\n",
    "## 5. **Numerical Stability**\n",
    "```python\n",
    "# Normal equation can be numerically unstable\n",
    "# If X^T X is nearly singular (multicollinearity), \n",
    "# the inverse becomes unreliable\n",
    "```\n",
    "\n",
    "## 6. **Feature Scaling**\n",
    "- **Normal Equation**: Works regardless of feature scales\n",
    "- **Gradient Descent**: Requires feature scaling for optimal convergence\n",
    "\n",
    "## When to Use Each:\n",
    "\n",
    "**Use Normal Equation when:**\n",
    "- Small dataset (< 10,000 examples)\n",
    "- Few features (< 1,000)\n",
    "- Linear regression only\n",
    "- Exact solution needed\n",
    "\n",
    "**Use Gradient Descent when:**\n",
    "- Large datasets\n",
    "- Many features\n",
    "- Non-linear models\n",
    "- Online learning needed\n",
    "- Memory constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs linear regression using gradient descent. The function should take NumPy arrays X (features with a column of ones for the intercept) and y (target) as input, along with learning rate alpha and the number of iterations, and return the coefficients of the linear regression model as a NumPy array. Round your answer to four decimal places. -0.0 is a valid result for rounding a very small number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3]) \n",
    "alpha = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.array([0.1107, 0.9513])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def linear_regression_gd(X, y, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs linear regression using gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "        X : np.ndarray, shape (m, n)\n",
    "            Feature matrix with a column of ones for the intercept.\n",
    "        y : np.ndarray, shape (m,)\n",
    "            Target vector.\n",
    "        alpha : float\n",
    "            Learning rate.\n",
    "        iterations : int\n",
    "            Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        theta : np.ndarray\n",
    "            Coefficients of the linear regression model, rounded to 4 decimals.\n",
    "    \"\"\"\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "    \n",
    "    # Get dimensions\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initialize parameters\n",
    "    theta = np.zeros((n, 1))\n",
    "    \n",
    "    # Gradient descent\n",
    "    for _ in range(iterations):\n",
    "        h = X @ theta  # Hypothesis: X @ theta\n",
    "        gradient = (1/m) * (X.T @ (h - y))  # Gradient of cost function\n",
    "        theta = theta - alpha * gradient  # Update parameters\n",
    "    \n",
    "    # Return flattened and rounded coefficients\n",
    "    return np.round(theta.flatten(), 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs feature scaling on a dataset using both standardization and min-max normalization. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. It should return two 2D NumPy arrays: one scaled by standardization and one by min-max normalization. Make sure all results are rounded to the nearest 4th decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2], [3, 4], [5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.63299316, 1.63299316])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]] [[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "min_max_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n",
    "print(standardized_data, min_max_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to write a Python function that implements the k-Means clustering algorithm. This function should take specific inputs and produce a list of final centroids. k-Means clustering is a method used to partition n points into k clusters. The goal is to group similar points together and represent each group by its center (called the centroid).\n",
    "Function Inputs:\n",
    "\n",
    "    points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n",
    "    k: An integer representing the number of clusters to form\n",
    "    initial_centroids: A list of initial centroid points, each a tuple of coordinates\n",
    "    max_iterations: An integer representing the maximum number of iterations to perform\n",
    "\n",
    "Function Output:\n",
    "\n",
    "A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)]\n",
    "k = 2\n",
    "initial_centroids = [(1, 1), (10, 1)] \n",
    "max_iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(max_iterations):\n",
    "    clusters = [[] for _ in range(k)]\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array(points[0]) - np.array(initial_centroids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(max_iterations):\n",
    "    clusters = [[] for _ in range(k)]\n",
    "    for point in points:\n",
    "        distances = [np.linalg.norm(np.array(point) - np.array(centroid)) for centroid in initial_centroids]\n",
    "        cluster_index = np.argmin(distances)\n",
    "        clusters[cluster_index].append(point)\n",
    "    new_centroids = [np.mean(cluster, axis=0) for cluster in clusters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function to generate train and test splits for K-Fold Cross-Validation. Your task is to divide the dataset into k folds and return a list of train-test indices for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "y = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "k = 5\n",
    "shuffle = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(len(X))\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 1, 5, 0, 7, 2, 9, 4, 3, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.RandomState(seed= 42)\n",
    "rng.shuffle(indices)\n",
    "indices\n",
    "# We shuffle to ensure that each fold in cross-validation is representative of the overall dataset, preventing bias from any \n",
    "# inherent ordering in the data and improving the reliability of model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_sizes = np.full(k, len(X) // k, dtype=int)\n",
    "fold_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_sizes[:len(X) % k] += 1\n",
    "fold_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n",
    "    \n",
    "    assert len(X) == len(y)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    indices = np.arange(n_samples)\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(seed= random_seed)\n",
    "        rng.shuffle(indices)\n",
    "    \n",
    "    fold_sizes = np.full(k, n_samples // k, dtype=int)\n",
    "    fold_sizes[:n_samples % k] += 1\n",
    "\n",
    "    current_index = 0\n",
    "    splits = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start = current_index\n",
    "        end = current_index + fold_size\n",
    "        test_indices = indices[start:end]\n",
    "        train_indices = np.concatenate((indices[:start], indices[end:]))\n",
    "        splits.append((train_indices, test_indices))\n",
    "        current_index = end\n",
    "        \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2, 3, 4, 5, 6, 7, 8, 9]), array([0, 1])),\n",
       " (array([0, 1, 4, 5, 6, 7, 8, 9]), array([2, 3])),\n",
       " (array([0, 1, 2, 3, 6, 7, 8, 9]), array([4, 5])),\n",
       " (array([0, 1, 2, 3, 4, 5, 8, 9]), array([6, 7])),\n",
       " (array([0, 1, 2, 3, 4, 5, 6, 7]), array([8, 9]))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that performs Principal Component Analysis (PCA) from scratch. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. The function should standardize the dataset, compute the covariance matrix, find the eigenvalues and eigenvectors, and return the principal components (the eigenvectors corresponding to the largest eigenvalues). The function should also take an integer k as input, representing the number of principal components to return.\n",
    "\n",
    "\n",
    "    1- Standardize the Dataset: Ensure that each feature has a mean of 0 and a standard deviation of 1.\n",
    "    2- Compute the Covariance Matrix: Reflects how features vary together.\n",
    "    3- Find Eigenvalues and Eigenvectors: Solve the characteristic equation for the covariance matrix.\n",
    "    4- Select Principal Components: Choose eigenvectors (components) with the highest eigenvalues for dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 4.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean(data, axis=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5, 1.5],\n",
       "       [1.5, 1.5]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "data_centered = (data - mean) / std\n",
    "cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "top_k_indices = np.argsort(eigenvalues)[::-1][:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70710678, 0.70710678])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = eigenvectors[:, top_k_indices]\n",
    "principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that implements the decision tree learning algorithm for classification. The function should use recursive binary splitting based on entropy and information gain to build a decision tree. It should take a list of examples (each example is a dict of attribute-value pairs) and a list of attribute names as input, and return a nested dictionary representing the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "\n",
    "\n",
    "    1- Select Attribute: Choose the attribute with the highest information gain.\n",
    "    2- Split Dataset: Divide the dataset based on the values of the selected attribute.\n",
    "    3- Recursion: Repeat the process for each subset until:\n",
    "            All data is perfectly classified, or\n",
    "            No remaining attributes can be used to make a split.\n",
    "\n",
    "This recursive process continues until the decision tree can no longer be split further or all examples have been classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n",
    "                    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n",
    "                    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n",
    "                    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n",
    "                ]\n",
    "\n",
    "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
    "target_attr = 'PlayTennis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'No', 'Yes', 'Yes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [ex[target_attr] for ex in examples]\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(values)\n",
    "\n",
    "for count in counts.values():\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(examples, target_attr):\n",
    "    values = [ex[target_attr] for ex in examples]\n",
    "    total = len(values)\n",
    "    counts = Counter(values)\n",
    "    ent = 0.0\n",
    "\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log2(p)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(examples, target_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Overcast', 'Rain', 'Sunny'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = set(ex[attributes[0]] for ex in examples)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(examples, attr, target_attr):\n",
    "    total_entropy = entropy(examples, target_attr)\n",
    "    values = set(ex[attr] for ex in examples)\n",
    "    total = len(examples)\n",
    "    subset_entropy = 0.0\n",
    "    for v in values:\n",
    "        subset = [ex for ex in examples if ex[attr] == v]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def entropy(examples, target_attr):\n",
    "    values = [ex[target_attr] for ex in examples]\n",
    "    total = len(values)\n",
    "    counts = Counter(values)\n",
    "    ent = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def information_gain(examples, attr, target_attr):\n",
    "    total_entropy = entropy(examples, target_attr)\n",
    "    values = set(ex[attr] for ex in examples)\n",
    "    total = len(examples)\n",
    "    subset_entropy = 0.0\n",
    "    for v in values:\n",
    "        subset = [ex for ex in examples if ex[attr] == v]\n",
    "        weight = len(subset) / total\n",
    "        subset_entropy += weight * entropy(subset, target_attr)\n",
    "    return total_entropy - subset_entropy\n",
    "\n",
    "def majority_value(examples, target_attr):\n",
    "    values = [ex[target_attr] for ex in examples]\n",
    "    return Counter(values).most_common(1)[0][0]\n",
    "\n",
    "def all_same_class(examples, target_attr):\n",
    "    first = examples[0][target_attr]\n",
    "    return all(ex[target_attr] == first for ex in examples)\n",
    "\n",
    "def learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n",
    "    # Base cases\n",
    "    if not examples:\n",
    "        return None\n",
    "    if all_same_class(examples, target_attr):\n",
    "        return examples[0][target_attr]\n",
    "    if not attributes:\n",
    "        return majority_value(examples, target_attr)\n",
    "\n",
    "    # Choose best attribute\n",
    "    gains = [(attr, information_gain(examples, attr, target_attr)) for attr in attributes]\n",
    "    best_attr = max(gains, key=lambda x: x[1])[0]\n",
    "\n",
    "    tree = {best_attr: {}}\n",
    "    attr_values = set(ex[best_attr] for ex in examples)\n",
    "    for v in attr_values:\n",
    "        subset = [ex for ex in examples if ex[best_attr] == v]\n",
    "        if not subset:\n",
    "            subtree = majority_value(examples, target_attr)\n",
    "        else:\n",
    "            remaining_attrs = [a for a in attributes if a != best_attr]\n",
    "            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)\n",
    "        tree[best_attr][v] = subtree\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outlook': {'Sunny': 'No', 'Rain': 'Yes', 'Overcast': 'Yes'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_decision_tree(examples, attributes, 'PlayTennis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hot', 'Hot', 'Hot', 'Mild']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [ex['Temperature'] for ex in examples]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  # üìö Import the math module for mathematical functions (e.g., log2)\n",
    "from collections import Counter  # üìä Import Counter to count occurrences of items\n",
    "from typing import List, Dict, Any, Union  # üìù Import type hints for better code clarity\n",
    "\n",
    "def calculate_entropy(labels: List[Any]) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Shannon entropy of the list of labels.\n",
    "    Entropy formula: H(S) = -‚àë p_i * log2(p_i)\n",
    "    where p_i is the probability of class i.\n",
    "    \"\"\"\n",
    "    total = len(labels)  # üî¢ Total number of labels (N)\n",
    "    if total == 0:  # ‚ö†Ô∏è If there are no labels, entropy is 0\n",
    "        return 0.0\n",
    "    counts = Counter(labels)  # üßÆ Count occurrences of each label\n",
    "    entropy = 0.0  # üèÅ Initialize entropy to 0\n",
    "    for count in counts.values():  # üîÅ For each unique label count\n",
    "        p = count / total  # üìê Calculate probability p_i = count / N\n",
    "        if p > 0:  # üö¶ Only consider non-zero probabilities\n",
    "            entropy -= p * math.log2(p)  # ‚ûñ Add -p_i * log2(p_i) to entropy (Shannon entropy formula)\n",
    "    return entropy  # üéØ Return the computed entropy\n",
    "\n",
    "def calculate_information_gain(\n",
    "    examples: List[Dict[str, Any]],\n",
    "    attr: str,\n",
    "    target_attr: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute information gain for splitting `examples` on `attr` w.r.t. `target_attr`.\n",
    "    Information Gain formula: IG(S, A) = H(S) - ‚àë_v (|S_v|/|S|) * H(S_v)\n",
    "    where S is the set of examples, A is the attribute, v are possible values of A,\n",
    "    and S_v is the subset where A = v.\n",
    "    \"\"\"\n",
    "    total = len(examples)  # üî¢ Total number of examples (|S|)\n",
    "    if total == 0:  # ‚ö†Ô∏è If no examples, information gain is 0\n",
    "        return 0.0\n",
    "    labels = [ex[target_attr] for ex in examples]  # üè∑Ô∏è Extract all target labels\n",
    "    total_entropy = calculate_entropy(labels)  # üßÆ Compute H(S), the entropy before split\n",
    "    values = set(ex[attr] for ex in examples)  # üóÇÔ∏è Get all unique values of attribute A\n",
    "    subset_entropy = 0.0  # üèÅ Initialize weighted entropy after split\n",
    "    for v in values:  # üîÅ For each possible value v of attribute A\n",
    "        subset = [ex for ex in examples if ex[attr] == v]  # üì¶ Subset S_v where A = v\n",
    "        weight = len(subset) / total  # ‚öñÔ∏è Weight = |S_v| / |S|\n",
    "        subset_labels = [ex[target_attr] for ex in subset]  # üè∑Ô∏è Labels in subset S_v\n",
    "        subset_entropy += weight * calculate_entropy(subset_labels)  # ‚ûï Add weighted entropy: (|S_v|/|S|) * H(S_v)\n",
    "    return total_entropy - subset_entropy  # üèÜ Information gain: IG(S, A) = H(S) - weighted sum\n",
    "\n",
    "def majority_class(\n",
    "    examples: List[Dict[str, Any]],\n",
    "    target_attr: str\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Return the most common value of `target_attr` in `examples`.\n",
    "    (Used for majority voting in leaves or when no attributes left)\n",
    "    \"\"\"\n",
    "    labels = [ex[target_attr] for ex in examples]  # üè∑Ô∏è Extract all target labels\n",
    "    if not labels:  # ‚ö†Ô∏è If no labels, return None\n",
    "        return None\n",
    "    return Counter(labels).most_common(1)[0][0]  # ü•á Return the label with highest count\n",
    "\n",
    "def learn_decision_tree(\n",
    "    examples: List[Dict[str, Any]],\n",
    "    attributes: List[str],\n",
    "    target_attr: str\n",
    ") -> Union[Dict[str, Any], Any]:\n",
    "    \"\"\"\n",
    "    Learn a decision tree using the ID3 algorithm üå≥.\n",
    "    Returns either a nested dict representing the tree or a class label at the leaves.\n",
    "    \"\"\"\n",
    "    if not examples:  # üà≥ Base case: if no examples, return None\n",
    "        return None\n",
    "    labels = [ex[target_attr] for ex in examples]  # üè∑Ô∏è Get all target labels\n",
    "    if all(label == labels[0] for label in labels):  # ‚úÖ Base case: if all labels are the same, return that label (pure node)\n",
    "        return labels[0]\n",
    "    if not attributes:  # üèÅ Base case: if no attributes left, return majority class (majority voting)\n",
    "        return majority_class(examples, target_attr)\n",
    "    # üßÆ Compute information gain for each attribute (select best split)\n",
    "    gains = [(attr, calculate_information_gain(examples, attr, target_attr)) for attr in attributes]\n",
    "    best_attr = max(gains, key=lambda x: x[1])[0]  # ü•á Choose attribute with highest information gain\n",
    "    tree = {best_attr: {}}  # üå≥ Create a new decision node for best_attr\n",
    "    values = set(ex[best_attr] for ex in examples)  # üóÇÔ∏è Get all unique values for best_attr\n",
    "    for v in values:  # üîÅ For each value v of best_attr\n",
    "        subset = [ex for ex in examples if ex[best_attr] == v]  # üì¶ Subset where best_attr == v\n",
    "        if not subset:  # üà≥ If subset is empty, use majority class as leaf\n",
    "            subtree = majority_class(examples, target_attr)\n",
    "        else:\n",
    "            remaining_attrs = [a for a in attributes if a != best_attr]  # üìù Remove best_attr from attribute list\n",
    "            subtree = learn_decision_tree(subset, remaining_attrs, target_attr)  # üîÑ Recursively build subtree\n",
    "        tree[best_attr][v] = subtree  # üåø Attach subtree to the current node for value v\n",
    "    return tree  # üå≥ Return the constructed decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outlook': {'Sunny': 'No', 'Rain': 'Yes', 'Overcast': 'Yes'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_decision_tree(examples, attributes, 'PlayTennis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasos Kernel SVM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch. The function should take a dataset (as a 2D NumPy array where each row represents a data sample and each column represents a feature), a label vector (1D NumPy array where each entry corresponds to the label of the sample), and training parameters such as the choice of kernel (linear or RBF), regularization parameter (lambda), and the number of iterations. Note that while the original Pegasos algorithm is stochastic (it selects a single random sample at each step), this problem requires using all samples in every iteration (i.e., no random sampling). The function should perform binary classification and return the model's alpha coefficients and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2], [2, 3], [3, 1], [4, 1]]) \n",
    "labels = np.array([1, 1, -1, -1]) \n",
    "kernel = 'rbf' \n",
    "lambda_val = 0.01 \n",
    "iterations = 100\n",
    "sigma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deterministic Pegasos Algorithm Steps**\n",
    "\n",
    "**Given:**\n",
    "- Training samples $(\\mathbf{x}_i, y_i)$, with $y_i \\in \\{-1, 1\\}$\n",
    "- Kernel function $K$\n",
    "- Regularization parameter $\\lambda$\n",
    "- Total iterations $T$\n",
    "\n",
    "---\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. **Initialize:**  \n",
    "   $\\alpha_i = 0$ for all $i$, and bias $b = 0$.\n",
    "\n",
    "2. **For each iteration** $t = 1, 2, \\ldots, T$:\n",
    "    - (a) **Compute learning rate:**  \n",
    "      $\\eta_t = \\frac{1}{\\lambda t}$\n",
    "    - (b) **For each training sample** $(\\mathbf{x}_i, y_i)$:\n",
    "        - i. **Compute decision value:**  \n",
    "          $f(\\mathbf{x}_i) = \\sum_j \\alpha_j y_j K(\\mathbf{x}_j, \\mathbf{x}_i) + b$\n",
    "        - ii. **If margin constraint is violated** ($y_i f(\\mathbf{x}_i) < 1$), **update:**\n",
    "            - $\\alpha_i \\leftarrow \\alpha_i + \\eta_t (y_i - \\lambda \\alpha_i)$\n",
    "            - $b \\leftarrow b + \\eta_t y_i$\n",
    "\n",
    "---\n",
    "\n",
    "**Common kernel functions:**\n",
    "\n",
    "- **Linear kernel:**  \n",
    "  $K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y}$\n",
    "\n",
    "- **Radial Basis Function (RBF) kernel:**  \n",
    "  $K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{y}\\|^2}{2\\sigma^2}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_function(x, y, kernel='linear', sigma=1.0):\n",
    "    if kernel == 'linear':\n",
    "        return np.dot(x, y)\n",
    "    elif kernel == 'rbf':\n",
    "        return np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported kernel: {kernel}\")\n",
    "\n",
    "def pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0) -> (list, float):\n",
    "    n_samples = len(data)\n",
    "    alphas = [0.0] * n_samples\n",
    "    b = 0.0\n",
    "\n",
    "    for t in range(1, iterations + 1):\n",
    "        eta_t = 1 / (lambda_val * t)\n",
    "        for i in range(n_samples):\n",
    "            x_i = data[i]\n",
    "            y_i = labels[i]\n",
    "            # Use the correct kernel function\n",
    "            def kf(x, y):\n",
    "                return kernel_function(x, y, kernel=kernel, sigma=sigma)\n",
    "            # Compute decision value\n",
    "            decision_value = sum(alphas[j] * labels[j] * kf(data[j], x_i) for j in range(n_samples)) + b\n",
    "            if y_i * decision_value < 1:\n",
    "                # Update alpha_i and b as per the deterministic Pegasos update\n",
    "                alphas[i] += eta_t * (y_i - lambda_val * alphas[i])\n",
    "                b += eta_t * y_i\n",
    "            # No else branch: only update when margin is violated\n",
    "\n",
    "    # For output formatting, round b to 4 decimals as in the expected output\n",
    "    b = round(b, 4)\n",
    "    # If all alphas are integer-valued, cast to int for output\n",
    "    alphas_out = [float(a) if abs(a - int(a)) > 1e-8 else int(a) for a in alphas]\n",
    "    return alphas_out, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation Function Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that computes the output of the sigmoid activation function given an input value z. The function should return the output rounded to four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def sigmoid(z: float) -> float:\n",
    "    result = 1 / (1 + math.exp(-z))\n",
    "    result = round(result, 4)\n",
    "    return result\n",
    "\n",
    "sigmoid(z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation Function Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that computes the softmax activation for a given list of scores. The function should return the softmax values as a list, each rounded to four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def softmax(scores: list[float]) -> list[float]:\n",
    "\t# Your code here\n",
    "\n",
    "    probabilities = []\n",
    "\n",
    "    sum_exp = sum(math.exp(score) for score in scores)\n",
    "\n",
    "    for z in scores:\n",
    "        softmax = math.exp(z) / sum_exp\n",
    "        softmax = round(softmax, 4)\n",
    "        probabilities.append(softmax) \n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features. The function should take a list of feature vectors (each vector representing multiple features for an example), associated true binary labels, and the neuron's weights (one for each feature) and bias as input. It should return the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]] \n",
    "labels = [0, 1, 0] \n",
    "weights = [0.7, -0.4] \n",
    "bias = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.050000000000000044"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(w * x for w, x in zip(weights, features[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n",
    "\t# Your code here\n",
    "\tprobabilities = []\n",
    "\tmse = 0\n",
    "\tfor feature, label in zip(features, labels):\n",
    "\t\tz = sum(w * x for w, x in zip(weights, feature)) + bias\n",
    "\t\tsigmoid = 1 / (1 + math.exp(-z))\n",
    "\t\tprobabilities.append(sigmoid)\n",
    "\t\tmse += (sigmoid - label) ** 2\n",
    "\tmse = mse / len(features)\n",
    "\treturn probabilities, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias. The function should take a list of feature vectors, associated true binary labels, initial weights, initial bias, a learning rate, and the number of epochs. The function should update the weights and bias using gradient descent based on the MSE loss, and return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Learning with Backpropagation\n",
    "\n",
    "This task involves implementing backpropagation for a single neuron in a neural network. The neuron processes inputs and updates its parameters to minimize the Mean Squared Error (MSE) between predicted outputs and true labels.\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "Compute the neuron's output by calculating the dot product of the weights and input features, then adding the bias:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "Apply the sigmoid activation function:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "### Loss Calculation (MSE)\n",
    "\n",
    "The Mean Squared Error quantifies the error between the neuron's predictions and the actual labels:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "### Backward Pass (Gradient Calculation)\n",
    "\n",
    "Compute the gradient of the MSE with respect to each weight and the bias. This involves the partial derivatives of the loss function with respect to the neuron's output, multiplied by the derivative of the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial w_j} = \\frac{2}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i) \\cdot \\sigma'(z_i) \\cdot x_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (\\sigma(z_i) - y_i) \\cdot \\sigma'(z_i)\n",
    "$$\n",
    "\n",
    "where $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$.\n",
    "\n",
    "### Parameter Update\n",
    "\n",
    "Update each weight and the bias by subtracting a portion of the gradient, determined by the learning rate $\\alpha$:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\frac{\\partial \\text{MSE}}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\alpha \\frac{\\partial \\text{MSE}}{\\partial b}\n",
    "$$\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "This process refines the neuron's ability to predict accurately by iteratively adjusting the weights and bias based on the error gradients, optimizing the neural network's performance over multiple iterations (epochs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n",
    "\t# Your code here\n",
    "\tweights = initial_weights.copy()\n",
    "\tbias = initial_bias\n",
    "\tmse_values = []\n",
    "\tn_samples = features.shape[0]\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tz = np.dot(features, weights) + bias\n",
    "\t\tsigmoid = 1 / (1 + np.exp(-z))\n",
    "\t\terrors = sigmoid - labels\n",
    "\t\tmse = np.mean(errors ** 2)\n",
    "\t\tmse_values.append(mse)\n",
    "\n",
    "\t\t# Gradients\n",
    "\t\tsigmoid_deriv = sigmoid * (1 - sigmoid)\n",
    "\t\tgrad_w = (2 / n_samples) * np.dot((errors * sigmoid_deriv), features)\n",
    "\t\tgrad_b = (2 / n_samples) * np.sum(errors * sigmoid_deriv)\n",
    "\n",
    "\t\t# Update\n",
    "\t\tweights = weights - learning_rate * grad_w\n",
    "\t\tbias = bias - learning_rate * grad_b\n",
    "\n",
    "\tupdated_weights = weights\n",
    "\tupdated_bias = bias\n",
    "\treturn updated_weights, updated_bias, mse_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Basic Autograd Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special thanks to Andrej Karpathy for making a video about this, if you haven't already check out his videos on YouTube https://youtu.be/VMj-3S1tku0?si=gjlnFP4o3JRN9dTg. Write a Python class similar to the provided 'Value' class that implements the basic autograd operations: addition, multiplication, and ReLU activation. The class should handle scalar values and should correctly compute gradients for these operations through automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=2, grad=0) Value(data=-3, grad=0) Value(data=10, grad=0) Value(data=-28, grad=0) Value(data=0, grad=1)\n"
     ]
    }
   ],
   "source": [
    "a = Value(2)\n",
    "b = Value(-3)\n",
    "c = Value(10)\n",
    "d = a + b * c\n",
    "e = d.relu()\n",
    "e.backward()\n",
    "print(a, b, c, d, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\tdef __init__(self, data, _children=(), _op=''):\n",
    "\t\tself.data = data  # üíæ Stores the scalar value (the number itself)\n",
    "\t\tself.grad = 0     # üßÆ Gradient of this value w.r.t. some scalar output (‚àÇoutput/‚àÇself), initialized to 0\n",
    "\t\tself._backward = lambda: None  # üîÑ Function to compute the local gradient and propagate it backward\n",
    "\t\tself._prev = set(_children)    # üå≥ Set of previous Value nodes (parents in the computation graph)\n",
    "\t\tself._op = _op                # üè∑Ô∏è Operation that produced this node (for graph visualization/debugging)\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"Value(data={self.data}, grad={self.grad})\"  # üñ®Ô∏è Nicely formats the Value for printing\n",
    "\n",
    "\tdef __add__(self, other):\n",
    "\t\t# ‚ûï Addition operation: z = x + y, dz/dx = 1, dz/dy = 1\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)  # üîÑ Ensure 'other' is a Value\n",
    "\t\tout = Value(self.data + other.data, (self, other), '+')      # üÜï Create new Value for the sum\n",
    "\t\tdef _backward():\n",
    "\t\t\t# üßÆ Chain rule: dL/dx += dL/dout * dout/dx = out.grad * 1\n",
    "\t\t\tself.grad += out.grad      # ‚àÇL/‚àÇself += ‚àÇL/‚àÇout * ‚àÇout/‚àÇself = out.grad * 1\n",
    "\t\t\tother.grad += out.grad    # ‚àÇL/‚àÇother += ‚àÇL/‚àÇout * ‚àÇout/‚àÇother = out.grad * 1\n",
    "\t\tout._backward = _backward     # üîó Attach backward function to the output node\n",
    "\t\treturn out\n",
    "\n",
    "\tdef __radd__(self, other):\n",
    "\t\t# ‚ûï Handles right-side addition (e.g., 2 + Value)\n",
    "\t\treturn self + other\n",
    "\n",
    "\tdef __mul__(self, other):\n",
    "\t\t# ‚úñÔ∏è Multiplication operation: z = x * y, dz/dx = y, dz/dy = x\n",
    "\t\tother = other if isinstance(other, Value) else Value(other)  # üîÑ Ensure 'other' is a Value\n",
    "\t\tout = Value(self.data * other.data, (self, other), '*')      # üÜï Create new Value for the product\n",
    "\t\tdef _backward():\n",
    "\t\t\t# üßÆ Chain rule: dL/dx += dL/dout * dout/dx = out.grad * y\n",
    "\t\t\tself.grad += other.data * out.grad   # ‚àÇL/‚àÇself += ‚àÇL/‚àÇout * ‚àÇout/‚àÇself = out.grad * other.data\n",
    "\t\t\tother.grad += self.data * out.grad   # ‚àÇL/‚àÇother += ‚àÇL/‚àÇout * ‚àÇout/‚àÇother = out.grad * self.data\n",
    "\t\tout._backward = _backward               # üîó Attach backward function to the output node\n",
    "\t\treturn out\n",
    "\n",
    "\tdef __rmul__(self, other):\n",
    "\t\t# ‚úñÔ∏è Handles right-side multiplication (e.g., 2 * Value)\n",
    "\t\treturn self * other\n",
    "\n",
    "\tdef relu(self):\n",
    "\t\t# üü© ReLU activation: f(x) = max(0, x), f'(x) = 1 if x > 0 else 0\n",
    "\t\tout = Value(max(0, self.data), (self,), 'ReLU')  # üÜï Create new Value for ReLU output\n",
    "\t\tdef _backward():\n",
    "\t\t\t# üßÆ Chain rule: dL/dx += dL/dout * dout/dx = out.grad * (self.data > 0)\n",
    "\t\t\tself.grad += (out.data > 0) * out.grad  # ‚àÇL/‚àÇself += ‚àÇL/‚àÇout * ‚àÇout/‚àÇself\n",
    "\t\tout._backward = _backward                  # üîó Attach backward function to the output node\n",
    "\t\treturn out\n",
    "\n",
    "\tdef backward(self):\n",
    "\t\t# üîÅ Backpropagation: computes gradients for all nodes in the graph\n",
    "\t\t# üìö Topological sort to ensure correct order of gradient computation\n",
    "\t\ttopo = []          # üìù List to store nodes in topological order\n",
    "\t\tvisited = set()    # ‚úÖ Set to keep track of visited nodes\n",
    "\t\tdef build_topo(v):\n",
    "\t\t\tif v not in visited:         # üö¶ If node not visited\n",
    "\t\t\t\tvisited.add(v)          # ‚úÖ Mark as visited\n",
    "\t\t\t\tfor child in v._prev:   # üë∂ Visit all parents (dependencies)\n",
    "\t\t\t\t\tbuild_topo(child)\n",
    "\t\t\t\ttopo.append(v)          # üì• Add node to topo list after its parents\n",
    "\t\tbuild_topo(self)                # üèÅ Start from self (output node)\n",
    "\n",
    "\t\tself.grad = 1                   # üéØ Seed the output node with gradient 1 (‚àÇoutput/‚àÇoutput = 1)\n",
    "\t\tfor node in reversed(topo):     # üîÑ Go in reverse topological order (from output to input)\n",
    "\t\t\tnode._backward()            # üßÆ Call each node's backward function to propagate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Matrix from Basis B to C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given basis vectors in two different bases B and C for R^3, write a Python function to compute the transformation matrix P from basis B to C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = [[1, 0, 0], \n",
    "    [0, 1, 0], \n",
    "    [0, 0, 1]]\n",
    "C = [[1, 2.3, 3], \n",
    "    [4.4, 25, 6], \n",
    "    [7.4, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n",
    "\n",
    "    P = np.linalg.inv(B) @ C\n",
    "\t\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_basis(B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Shuffle of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to perform a random shuffle of the samples in two numpy arrays, X and y, while maintaining the corresponding order between them. The function should have an optional seed parameter for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], \n",
    "                [3, 4], \n",
    "                [5, 6], \n",
    "                [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.arange(len(X))\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(indices)\n",
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[indices]\n",
    "y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3, 4],\n",
       "        [7, 8],\n",
       "        [1, 2],\n",
       "        [5, 6]]),\n",
       " array([2, 4, 1, 3]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffle_data(X, y, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    return X[indices], y[indices]\n",
    "shuffle_data(X, y, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Iterator for Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a batch iterable function that samples in a numpy array X and an optional numpy array y. The function should return batches of a specified size. If y is provided, the function should return batches of (X, y) pairs; otherwise, it should return batches of X only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], \n",
    "                [3, 4], \n",
    "                [5, 6], \n",
    "                [7, 8], \n",
    "                [9, 10]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "[[[[1, 2], [3, 4]], [1, 2]],\n",
    "     [[[5, 6], [7, 8]], [3, 4]],\n",
    "     [[[9, 10]], [5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\n"
     ]
    }
   ],
   "source": [
    "def batch_iterator(X, y=None, batch_size=64):\n",
    "    n_samples = len(X)\n",
    "    batches = []\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        if y is not None:\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            # Convert to list for pretty output\n",
    "            batches.append([X_batch.tolist(), y_batch.tolist()])\n",
    "        else:\n",
    "            batches.append(X_batch.tolist())\n",
    "    return batches\n",
    "\n",
    "print(batch_iterator(X, y, batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide Dataset Based on Feature Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to divide a dataset based on whether the value of a specified feature is greater than or equal to a given threshold. The function should return two subsets of the dataset: one with samples that meet the condition and another with samples that do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2], \n",
    "                [3, 4], \n",
    "                [5, 6], \n",
    "                [7, 8], \n",
    "                [9, 10]])\n",
    "feature_i = 0\n",
    "threshold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function divide_on_feature.<locals>.<lambda> at 0x760e8d9468e0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 5,  6],\n",
       "        [ 7,  8],\n",
       "        [ 9, 10]]),\n",
       " array([[1, 2],\n",
       "        [3, 4]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda x: x[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda x: x[feature_i] == threshold\n",
    "    X_1 = np.array([x for x in X if split_func(x)])\n",
    "    X_2 = np.array([x for x in X if not split_func(x)])\n",
    "    return X_1, X_2\n",
    "divide_on_feature(X, feature_i, threshold)\n",
    "\n",
    "## Split Dataset into Training and Validation Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sorted Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function that takes a 2-D NumPy array X and an integer degree, generates all polynomial feature combinations of the columns of X up to the given degree inclusive, then sorts the resulting features for each sample from lowest to highest value. The function should return a new 2-D NumPy array whose rows correspond to the input samples and whose columns are the ascending-sorted polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2, 3],\n",
    "              [3, 4],\n",
    "              [5, 6]])\n",
    "degree = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "(0,)\n",
      "(1,)\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  3.,  4.,  9., 12., 16.],\n",
       "       [ 1.,  5.,  6., 25., 30., 36.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def polynomial_features(X, degree):\n",
    "    n_samples, n_features = X.shape  # üßÆ Get the number of samples and features from X\n",
    "    features = []  # üì¶ Initialize a list to store all generated features\n",
    "    \n",
    "    for d in range(degree + 1):  # üîÅ Loop over all degrees from 0 up to the specified degree\n",
    "        for comb in combinations_with_replacement(range(n_features), d):  # üß© Generate all combinations (with replacement) of feature indices for current degree\n",
    "            print(comb)\n",
    "            feature = np.ones(n_samples)  # üèÅ Start with an array of ones for multiplication (for each sample)\n",
    "            for idx in comb:  # üî¢ For each index in the current combination\n",
    "                feature *= X[:, idx]  # ‚úñÔ∏è Multiply the corresponding feature column for all samples\n",
    "            features.append(feature)  # ‚ûï Add the computed feature to the list\n",
    "    \n",
    "    # Stack all features and sort each row\n",
    "    result = np.column_stack(features)  # üèóÔ∏è Stack all features as columns to form the final feature matrix\n",
    "    result.sort(axis=1)  # üìä Sort each row in ascending order (so features for each sample are sorted)\n",
    "    \n",
    "    \n",
    "    return result\n",
    "polynomial_features(X, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Subsets of a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Python function to generate random subsets of a given dataset. The function should take in a 2D numpy array X, a 1D numpy array y, an integer n_subsets, and a boolean replacements. It should return a list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset). If replacements is True, the subsets should be created with replacements; otherwise, without replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 2],\n",
    "                [3, 4],\n",
    "                [5, 6],\n",
    "                [7, 8],\n",
    "                [9, 10]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "n_subsets = 3\n",
    "replacements = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n",
    "\tnp.random.seed(seed)\n",
    "\t\n",
    "\t# Validate inputs\n",
    "\tif X.shape[0] != y.shape[0]:\n",
    "\t\traise ValueError(\"X and y must have the same number of samples\")\n",
    "\t\n",
    "\tn_samples = X.shape[0]\n",
    "\t\n",
    "\t# Set subset size based on replacements parameter\n",
    "\tif replacements:\n",
    "\t\tsubset_size = n_samples  # Full dataset size for sampling with replacement\n",
    "\telse:\n",
    "\t\tsubset_size = 2  # Fixed size of 2 for sampling without replacement\n",
    "\t\n",
    "\t# Validate subset size for sampling without replacement\n",
    "\tif not replacements and subset_size > n_samples:\n",
    "\t\traise ValueError(\"subset_size cannot be larger than dataset size when replacements=False\")\n",
    "\t\n",
    "\tsubsets = []\n",
    "\t\n",
    "\tfor _ in range(n_subsets):\n",
    "\t\tif replacements:\n",
    "\t\t\t# Sample with replacement\n",
    "\t\t\tindices = np.random.choice(n_samples, size=subset_size, replace=True)\n",
    "\t\telse:\n",
    "\t\t\t# Sample without replacement\n",
    "\t\t\tindices = np.random.choice(n_samples, size=subset_size, replace=False)\n",
    "\t\t\n",
    "\t\t# Create subset using the selected indices\n",
    "\t\tX_subset = X[indices]\n",
    "\t\ty_subset = y[indices]\n",
    "\t\t\n",
    "\t\tsubsets.append((X_subset, y_subset))\n",
    "\t\n",
    "\treturn subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 7,  8],\n",
      "       [ 9, 10],\n",
      "       [ 5,  6],\n",
      "       [ 9, 10],\n",
      "       [ 9, 10]]), array([4, 5, 3, 5, 5])), (array([[ 3,  4],\n",
      "       [ 5,  6],\n",
      "       [ 5,  6],\n",
      "       [ 5,  6],\n",
      "       [ 9, 10]]), array([2, 3, 3, 3, 5])), (array([[ 7,  8],\n",
      "       [ 5,  6],\n",
      "       [ 9, 10],\n",
      "       [ 3,  4],\n",
      "       [ 7,  8]]), array([4, 3, 5, 2, 4]))]\n"
     ]
    }
   ],
   "source": [
    "print(get_random_subsets(X,y, 3, True, seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé≤ **What does `get_random_subsets` do?**\n",
    "\n",
    "The function `get_random_subsets(X, y, n_subsets, replacements=True, seed=42)` generates multiple random subsets (mini-datasets) from your original dataset `(X, y)`. \n",
    "\n",
    "**Formulation:**\n",
    "- Given a dataset with $n$ samples: $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^n$\n",
    "- For each subset $i$ (where $i = 1, 2, ..., n_{\\text{subsets}}$):\n",
    "  - Randomly select a set of indices $S_i$ of size $k$:\n",
    "    - If `replacements=True`: $k = n$ and sampling is *with* replacement (bootstrap sampling)\n",
    "    - If `replacements=False`: $k = 2$ and sampling is *without* replacement\n",
    "  - Form subset: $(X_{S_i}, y_{S_i})$\n",
    "- Return a list of all such $(X_{S_i}, y_{S_i})$ pairs\n",
    "\n",
    "**In other words:**\n",
    "- üß∫ If `replacements=True`, each subset is a \"bag\" of $n$ samples drawn with replacement (like bootstrapping).\n",
    "- üßë‚Äçü§ù‚Äçüßë If `replacements=False`, each subset is a pair of samples drawn without replacement.\n",
    "\n",
    "**Why?**\n",
    "- This is useful for ensemble methods (like bagging) or for creating random pairs for algorithms like random forests or similarity learning.\n",
    "\n",
    "**Summary Table:**\n",
    "| Parameter         | Meaning                                      |\n",
    "|-------------------|----------------------------------------------|\n",
    "| X, y              | Input data and labels                        |\n",
    "| n_subsets         | How many random subsets to generate          |\n",
    "| replacements      | Sample with replacement? (True/False)        |\n",
    "| seed              | Random seed for reproducibility              |\n",
    "\n",
    "üöÄ The function returns:  \n",
    "`[(X_subset_1, y_subset_1), (X_subset_2, y_subset_2), ..., (X_subset_n, y_subset_n)]`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
